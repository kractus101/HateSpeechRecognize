# -*- coding: utf-8 -*-
"""BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RvJCOgd-az63FjjwmE_J3zlOK3N0qYVW

# Download/Import packages and libraries
"""

!pip install transformers torch
from google.colab import drive
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from torch.utils.data import DataLoader, Dataset
import torch
import re
import string
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stopword=set(stopwords.words('english'))
stemmer = nltk.SnowballStemmer("english")
import matplotlib.pyplot as plt
!pip install scikit-plot
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix
import scikitplot as skplt

drive.mount("/content/drive") # establishing a connecttion between google drive

data = pd.read_csv("/content/drive/MyDrive/Data/labeled_data.csv") # Read the datset from google drive
data.head(5)

data = data.dropna(subset=['class']) #dropping rows with missing value
data.head()

unique_counts = data['class'].value_counts() #checking the balance of the dataset

# Display the result
print(unique_counts)

"""#Text cleaning"""

def clean(text):
    text = str(text).lower()           # convert to lower text
    text = re.sub('\[.*?\]', '', text) #remove special characters specified
    text = re.sub('https?://\S+|www\.\S+', '', text) #remove URL and website addresses
    text = re.sub('<.*?>+', '', text) #Remove HTML tags
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # Remove punctuation characters
    text = re.sub('\n', '', text) # Remove newline characters
    text = re.sub('\w*\d\w*', '', text)   # Remove words containing digits
    text = [word for word in text.split(' ') if word not in stopword] #remove stop words
    text = " ".join(text) #join the list of words into a single string
    text = [stemmer.stem(word) for word in text.split(' ')] #apply stemming
    text = " ".join(text) #Join the list of stemmed words into a single string
    return text

data["tweet"] = data["tweet"].apply(clean)

data.head()

"""# Split the data into training and validation sets"""

train_texts, val_texts, train_labels, val_labels = train_test_split(data['tweet'], data['class'], test_size=0.2, random_state=42)

"""# Tokenize and create datasets"""

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') #loading tokenizer for bert-base-uncased model

# This PyTorch dataset is created to be used along with PyTorch's "Dataloader" to efficiently load and batch the data
class HateSpeechDataset(Dataset): # Defining a class called HateSpeechDataset that inherits from PyTorch's 'Dataset' class
    def __init__(self, texts, labels, tokenizer, max_len=128):   # max_len: The maximum length of the tokenized sequences
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self): # Returns the total number of samples in the dataset, which is the length of the texts.
        return len(self.texts)

    def __getitem__(self, idx):  # retrieves a single sample from the dataset at the specified index idx
        text = str(self.texts.iloc[idx]) # retrieve text at index idx
        label = int(self.labels.iloc[idx]) # retrieve label at index idx
        encoding = self.tokenizer.encode_plus(   # uses the tokenizer specified to tokenize and save it in 'encoding' dictionary
            text,                         # input text at index idx that needs to be tokenized
            add_special_tokens=True,     # adding special tokens like [CLS] and [SEP]
            max_length=self.max_len,     # specifying the maximum length of tokenized sequence. It will truncate or do padding to maintain this length
            return_token_type_ids=False, # for text classification we don't need the token type id, hence false
            padding='max_length',       # do padding if length less than max_length
            return_attention_mask=True,
            return_tensors='pt',     # specifying the output should be PyTorch tensors
            truncation=True,         # truncate if length greater than max_length
        )

        return {     # returns a dictionary containing relevant information for the dataset sample at index idx
            'text': text,                                              #original text
            'input_ids': encoding['input_ids'].flatten(),              # tokenized input id's
            'attention_mask': encoding['attention_mask'].flatten(),    # attention mask
            'label': torch.tensor(label, dtype=torch.long)             # label
        }

"""# Load BERT model for sequence classification"""

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3) # 3 is the number of classes in dataset

"""Train and validate the model for different combinations of Learning rate and batch size"""

# learning_rates = [2e-5, 3e-5, 5e-5]
# batch_sizes = [8, 16]

# for learning_rate in learning_rates:
#     for batch_size in batch_sizes:
#         print(f"\nTraining with Learning Rate: {learning_rate}, Batch Size: {batch_size}\n")

#         train_dataset = HateSpeechDataset(train_texts, train_labels, tokenizer)
#         val_dataset = HateSpeechDataset(val_texts, val_labels, tokenizer)

#         train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
#         val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

#         model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)
#         device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#         model.to(device)
#         optimizer = AdamW(model.parameters(), lr=learning_rate)

#         num_epochs = 3
#         for epoch in range(num_epochs):
#             model.train()
#             total_loss = 0.0
#             correct_preds_train = 0
#             total_preds_train = 0

#             for batch in train_dataloader:
#                 input_ids = batch['input_ids'].to(device)
#                 attention_mask = batch['attention_mask'].to(device)
#                 labels = batch['label'].to(device)

#                 optimizer.zero_grad()

#                 outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
#                 loss = outputs.loss
#                 total_loss += loss.item()

#                 _, preds = torch.max(outputs.logits, dim=1)
#                 correct_preds_train += torch.sum(preds == labels).item()
#                 total_preds_train += len(labels)

#                 loss.backward()
#                 optimizer.step()

#             avg_train_loss = total_loss / len(train_dataloader)
#             train_accuracy = correct_preds_train / total_preds_train

#             # Validation
#             model.eval()
#             val_loss = 0.0
#             correct_preds_val = 0
#             total_preds_val = 0

#             with torch.no_grad():
#                 for batch in val_dataloader:
#                     input_ids = batch['input_ids'].to(device)
#                     attention_mask = batch['attention_mask'].to(device)
#                     labels = batch['label'].to(device)

#                     outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
#                     val_loss += outputs.loss.item()

#                     _, preds = torch.max(outputs.logits, dim=1)
#                     correct_preds_val += torch.sum(preds == labels).item()
#                     total_preds_val += len(labels)

#             avg_val_loss = val_loss / len(val_dataloader)
#             val_accuracy = correct_preds_val / total_preds_val

#             print(f'Epoch {epoch + 1}/{num_epochs}, '
#                   f'Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
#                   f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')

"""#Train the model for BS =16 anf LR =2e-5

Create datasets and dataloaders
"""

train_dataset = HateSpeechDataset(train_texts, train_labels, tokenizer)  #create training dataset with HateSpeechḌāṭāśet class
val_dataset = HateSpeechDataset(val_texts, val_labels, tokenizer)  # create validation dataset with HateSpeechḌāṭāśet class

# DataLoader is used to efficiently load and iterate over batches of data during training or evaluation.
train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Create a DataLoader for the training dataset.
val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)  # Create a DataLoader for the validation dataset

"""Training the model"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  #check if GPU (cuda) is available, otherwise use CPU
model.to(device)   # moving the model to the device (GPU/CPU)
optimizer = AdamW(model.parameters(), lr=2e-5)  #Defineing the AdamW optimizer with learning rate 2e-5
num_epochs = 3    # defining number of traing epochs. one epoch means going through the whole datset once
for epoch in range(num_epochs):       # Loop over the specified number of epochs for training
    model.train()   #set the model to traing mode
    total_loss = 0.0   # Initialize variables to track training loss and accuracy
    correct_preds_train = 0
    total_preds_train = 0

    for batch in train_dataloader:      # Iterate over batches in the training dataloader
        input_ids = batch['input_ids'].to(device)   # Move batch data to the specified device
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        optimizer.zero_grad()  # Zero the gradients to prevent accumulation from previous batches

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels) # Forward pass: compute model outputs and loss
        loss = outputs.loss
        total_loss += loss.item()

        _, preds = torch.max(outputs.logits, dim=1)   # Calculate training accuracy
        correct_preds_train += torch.sum(preds == labels).item()
        total_preds_train += len(labels)

        loss.backward()   # Backward pass: compute gradients and update model parameters
        optimizer.step()

    avg_train_loss = total_loss / len(train_dataloader)   # Calculate average training loss and accuracy
    train_accuracy = correct_preds_train / total_preds_train

    # Validation
    model.eval()   # Set the model in evaluation mode (no gradient computation)
    val_loss = 0.0  # Initialize variables to track validation loss and accuracy
    correct_preds_val = 0
    total_preds_val = 0

    with torch.no_grad():    # Iterate over batches in the validation dataloader
        for batch in val_dataloader:
            input_ids = batch['input_ids'].to(device)  # Move batch data to the specified device
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)  # Forward pass: compute model outputs and loss
            val_loss += outputs.loss.item()

            _, preds = torch.max(outputs.logits, dim=1)  # Calculate validation accuracy
            correct_preds_val += torch.sum(preds == labels).item()
            total_preds_val += len(labels)

    avg_val_loss = val_loss / len(val_dataloader)  # Calculate average validation loss and accuracy
    val_accuracy = correct_preds_val / total_preds_val

    print(f'Epoch {epoch + 1}/{num_epochs}, ' # Print epoch-wise training and validation metrics
          f'Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')

"""# Calculate precision, recall, and F1 score"""

model.eval()
all_preds = []   # Initialize empty lists to store all predictions
all_labels = []  # Initialize empty lists to store all labels

with torch.no_grad():
    for batch in val_dataloader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        outputs = model(input_ids, attention_mask=attention_mask)
        _, preds = torch.max(outputs.logits, dim=1)

        all_preds.extend(preds.cpu().numpy())   # Extend the lists with predicted labels
        all_labels.extend(labels.cpu().numpy()) # Extend the lists with true labels

precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted') # Calculate precision, recall, F1 score,

print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}')  # print the calculated metrics

"""# Calculate and plot confusion matrix"""

conf_matrix = confusion_matrix(all_labels, all_preds)  # calculate the confussion matrix
print("Confusion Matrix:")
skplt.metrics.plot_confusion_matrix(all_labels, all_preds, normalize=True) # plot the confusion matrix
plt.show()